# Evaluation & Datasets

See also:

- From https://github.com/imaurer/awesome-decentralized-llm#leaderboards:
  + [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
  + [AlpacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval)
  + [Code Generation on HumanEval](https://paperswithcode.com/sota/code-generation-on-humaneval)
- Quality: ["Better Data = Better Performance"](https://cameronrwolfe.substack.com/i/135439692/better-data-better-performance)
- https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e#evaluation

## LLM Leaderboard criticism links to maybe mention

- "Optimizing models for LLM Leaderboard is a HUGE mistake" (weird that a "good" model means ranking high in 4 different relatively controversial benchmarking suites) https://www.reddit.com/r/LocalLLaMA/comments/15n6cmb/optimizing_models_for_llm_leaderboard_is_a_huge
- related: "Classifier Technology and the Illusion of Progress" https://arxiv.org/abs/math/0606441
- sceptical about value of testing LLMs without context (real value of LLMs currently is fine-tuning/giving context so they get good task-specific performance... And we can compare different LLMs' ability to deal with said context. LLM leaderboard wants to test AGI not AI, which is not what any current model is designed to do) https://github.com/premAI-io/dev-portal/pull/53#discussion_r1293847469
- https://dev.premai.io/blog/evaluating-open-source-llms

## Perplexity

probably put short definition in [](glossary)?

## Reinforcement Learning for a Chat-based LLM

{{ comments }}
